# -*- coding: utf-8 -*-
"""diabetes_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19LQGtkRTeMLcD_cdWLyRkMxTts4T66T9
"""

import pandas as pd
# Load the DataFrame from the .CSV file
patients_data = pd.read_csv('diabetes.csv')

# a-1) overal structure of the data
# Display the information about the dataset
print(patients_data.info())
# Show descriptive statistics
print(patients_data.describe())

# a-2) missing values
missing_values_count = patients_data.isna().sum()
total_rows = len(patients_data)
missing_values_portion = (missing_values_count / total_rows)*100

print("Number of Missing Values per Column:")
print(missing_values_count)
print("\nPortion of Missing Values per Column (%)")
print(missing_values_portion)

import matplotlib.pyplot as plt
import seaborn as sns
# a-3) correlation matrix
correlation_matrix = patients_data.corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True,
            fmt=".2f", cmap='coolwarm', cbar=True)
plt.title('Correlation Matrix')
plt.show()

# a-4) Observations
# Specifing the columns of interest
cols = ['Pregnancies', 'BMI', 'DiabetesPedigreeFunction', 'Age']

# Creating figures for each column
for column in cols:
    plt.figure()
    value_counts = patients_data[column].value_counts().sort_index()

    plt.bar(value_counts.index.astype(str),
            value_counts.values, color='skyblue')
    plt.title(f'Frequency of Each Specific Numerical Value in {column}')
    plt.xlabel('Unique Values')
    plt.ylabel('Frequency')
    plt.show()
    print("\n")

# Histogram

import numpy as np
# a - 5) scatter and hexbin
features = patients_data.columns.tolist()
outcome = 'Outcome'
features.remove(outcome)
# Create a figure for each pair of features
for i in range(0, len(features), 2):
    fig, axs = plt.subplots(2, 2, figsize=(15, 10))
    axs = axs.flatten()

    # Assign each subplot a specific plot
    for j in range(4):
        ax = axs[j]
        if j % 2 == 0:  # Even index: scatter plot
            feature = features[i + j // 2]
            ax.scatter(patients_data[feature],
                       patients_data[outcome], alpha=0.5)
            ax.set_title(f'Scatter Plot: {feature} vs {outcome}')
        else:  # Odd index: hexbin plot
            feature = features[i + (j - 1) // 2]
            hb = ax.hexbin(patients_data[feature], patients_data[outcome],
                           gridsize=30, cmap='Blues', reduce_C_function=np.mean)
            fig.colorbar(hb, ax=ax, orientation='vertical',
                         label='Mean in bin')
            ax.set_title(f'Hexbin Plot: {feature} vs {outcome}')

        ax.set_xlabel(feature)
        ax.set_ylabel(outcome)

    plt.tight_layout()
    plt.show()
    print("\n")

# b-1) missing values
# outliers and unacceptable value

def adjust_values(x):
    if x <= 0:
        return np.nan
    else:
        return x

patients_data.iloc[:, :8] = patients_data.iloc[:, :8].applymap(adjust_values)
missing_values_count = patients_data.isna().sum()

# drop columns
columns_to_drop = ['Insulin', 'SkinThickness']
for column in columns_to_drop:
    patients_data.drop(column, axis=1, inplace=True)

features = patients_data.columns.tolist()
features.remove(outcome)
# print(features)
# imputaion with indicator variable
for feature in features:
    patients_data[feature + '_missing'] = patients_data[feature].isna()

    median_value = patients_data[feature].median()
    mean_value = patients_data[feature].mean()
    # print(f"{feature} -> mean : {mean_value:.2f}, median : {median_value:.2f}")
    patients_data[feature] = patients_data[feature].fillna(median_value)

# b - 2)
# Data Distribution
patients_data[features].hist(bins=50, figsize=(20, 15))
plt.show()

from scipy.stats import shapiro
# Statistical Test for Normality
for column in patients_data.columns:
    stat, p = shapiro(patients_data[column].dropna())
    alpha = 0.05
    if p > alpha:
        print(f"{column} : Data looks Gaussian, p = {p}")
    else:
        print(f"{column} : Data does not look Gaussian, p = {p}")

decisions = {}
for column in patients_data.columns:
    stat, p = shapiro(patients_data[column].dropna())
    if p > 0.05 and patients_data[column].min() >= 0:
        decisions[column] = 'Standardize'
    elif patients_data[column].min() < 0 or patients_data[column].max() > 1:
        decisions[column] = 'Normalize'
    else:
        decisions[column] = 'None needed'

print(decisions)

from sklearn.preprocessing import MinMaxScaler
# normalize
features_to_normalize = patients_data[[
    'Pregnancies', 'Glucose', 'BloodPressure', 'BMI', 'DiabetesPedigreeFunction', 'Age']]
scaler = MinMaxScaler()
normalized_features = scaler.fit_transform(features_to_normalize)
patients_data[['Pregnancies', 'Glucose', 'BloodPressure', 'BMI',
               'DiabetesPedigreeFunction', 'Age']] = normalized_features

patients_data.to_csv('normalized_data.csv', index=False)

from sklearn.model_selection import train_test_split
# c - 1, 2)
X = patients_data.drop(patients_data.columns[6], axis=1)
Y = patients_data[patients_data.columns[6]]

X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.20, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score

# Initialize the Models
models = {
    "Logistic Regression": LogisticRegression(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Support Vector Machine": SVC()
}

# train the models
for name, model in models.items():
    model.fit(X_train, Y_train)
# Evaluate
results = {}
for name, model in models.items():
    # Predict on the testing set
    Y_pred = model.predict(X_test)
    # Calculate confusion matrix and accuracy
    cm = confusion_matrix(Y_test, Y_pred)
    accuracy = accuracy_score(Y_test, Y_pred)
    # Store results in the dictionary
    results[name] = {'Confusion Matrix': cm, 'Accuracy': accuracy}
    # Print results
    print(f"\n{name}:")
    print("Confusion Matrix:")
    print(cm)
    print(f"Accuracy : {100*accuracy:.2f}%")

# Initialize the Models 2
models = {
    "Logistic Regression": LogisticRegression(max_iter=300, penalty='l2'),
    "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=7),
    "Decision Tree": DecisionTreeClassifier(criterion='entropy', max_depth=3),
    "Random Forest": RandomForestClassifier(n_estimators=200, max_features=0.5),
    "Support Vector Machine": SVC(kernel='linear', degree=4)
}

# train the models
for name, model in models.items():
    model.fit(X_train, Y_train)
    # print(f"{name} trained!")


# Evaluate
results = {}

for name, model in models.items():
    # Predict on the testing set
    Y_pred = model.predict(X_test)

    # Calculate confusion matrix and accuracy
    cm = confusion_matrix(Y_test, Y_pred)
    accuracy = accuracy_score(Y_test, Y_pred)

    # Store results in the dictionary
    results[name] = {'Confusion Matrix': cm, 'Accuracy': accuracy}

    # Print results
    print(f"\n{name}:")
    print("Confusion Matrix:")
    print(cm)
    print(f"Accuracy : {100*accuracy:.2f}%")

from sklearn.model_selection import GridSearchCV
# GridSearchCV
# LR
param_grid = [
    {'solver': ['liblinear'], 'penalty': [
        'l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100]},
    {'solver': ['lbfgs', 'newton-cg'],
        'penalty': ['l2', None], 'C': [0.01, 0.1, 1, 10, 100]},
    {'solver': ['saga'], 'penalty': ['l1', 'l2', 'elasticnet'],
        'C': [0.01, 0.1, 1, 10, 100], 'l1_ratio': [0.1, 0.5, 0.7]},
    {'solver': ['saga', 'lbfgs', 'newton-cg'],
        'penalty': [None], 'C': [0.01, 0.1, 1, 10, 100]}
]
# Set up the model and GridSearchCV parameters
model = LogisticRegression(max_iter=10000)
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, verbose=2, n_jobs=-1, scoring='accuracy')
# Fit GridSearchCV to the training data
grid_search.fit(X_train, Y_train)
# Output the best parameters and the best score
print("Best parameters found: ", grid_search.best_params_)
# print("Best cross-validation score: {:.2f}".format(grid_search.best_score_))
# Evaluate the best model on the test data
# Predict on the test data using the best model
Y_pred = grid_search.best_estimator_.predict(X_test)
acc = 100*accuracy_score(Y_test, Y_pred)
print(f"Test accuracy: {acc:.2f}%")
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, verbose=0, n_jobs=-1, scoring='accuracy', error_score='raise')

# KNN
param_grid = {
    'n_neighbors': [3, 5, 10, 15, 20],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski'],
    'leaf_size': [30, 40, 50],
    'p': [1, 2]  # 1 for manhattan, 2 for euclidean/minkowski
}
knn = KNeighborsClassifier()
grid_search_knn = GridSearchCV(
    estimator=knn, param_grid=param_grid, cv=5, verbose=0, n_jobs=-1, scoring='accuracy')
grid_search_knn.fit(X_train, Y_train)
print("Best parameters found: ", grid_search_knn.best_params_)
best_knn = grid_search_knn.best_estimator_
# Evaluate the best model on the test data
Y_pred = best_knn.predict(X_test)
acc = 100*accuracy_score(Y_test, Y_pred)
print(f"Test accuracy: {acc:.2f}%")

# Decision Tree
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 1, 3, 10, 20, 30, 50],
    'min_samples_split': [2, 10, 20],
    'min_samples_leaf': [1, 5, 10],
    'max_features': [None, 'sqrt', 'log2']
}
dt = DecisionTreeClassifier(random_state=42)
grid_search_dt = GridSearchCV(
    estimator=dt, param_grid=param_grid, cv=5, verbose=0, n_jobs=-1, scoring='accuracy')
grid_search_dt.fit(X_train, Y_train)
print("Best parameters found: ", grid_search_dt.best_params_)
best_dt = grid_search_dt.best_estimator_
# Predict on the test data using the best model
Y_pred = best_dt.predict(X_test)
acc = 100*accuracy_score(Y_test, Y_pred)
print(f"Test accuracy: {acc:.2f}%")

# RF
param_grid = {
    'n_estimators': [100, 200, 300],  # Number of trees in the forest
    # Function to measure the quality of a split
    'criterion': ['gini', 'entropy'],
    # Maximum number of levels in each decision tree
    'max_depth': [None, 10, 20, 30],
    # Minimum number of data points placed in a node before the node is split
    'min_samples_split': [2, 10, 20],
    # Minimum number of data points allowed in a leaf node
    'min_samples_leaf': [1, 5, 10],
    # Number of features to consider when looking for the best split
    'max_features': ['sqrt', 'log2'],
    # Method for sampling data points (with or without replacement)
    'bootstrap': [True, False]
}
rf = RandomForestClassifier(random_state=42)
grid_search_rf = GridSearchCV(
    estimator=rf, param_grid=param_grid, cv=5, verbose=0, n_jobs=-1, scoring='accuracy')
grid_search_rf.fit(X_train, Y_train)
print("Best parameters found: ", grid_search_rf.best_params_)
best_rf = grid_search_rf.best_estimator_
# Evaluating the best model on the test data
Y_pred = best_rf.predict(X_test)
acc = 100*accuracy_score(Y_test, Y_pred)
print(f"Test accuracy: {acc:.2f}%")

# SVM
param_grid = {
    'C': [0.01, 0.1, 0.5, 1, 10, 100],  # Regularization parameter
    # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'
    'gamma': [0.1, 0.5, 1, 5, 0.01, 0.001],
    'kernel': ['rbf', 'poly', 'sigmoid', 'linear']  # Type of SVM kernel
}
svm = SVC(random_state=42)
grid_search_svm = GridSearchCV(
    estimator=svm, param_grid=param_grid, cv=5, verbose=0, n_jobs=-1, scoring='accuracy')
grid_search_svm.fit(X_train, Y_train)
print("Best parameters found: ", grid_search_svm.best_params_)
best_svm = grid_search_svm.best_estimator_
# Predict on the test data using the best model
Y_pred = best_svm.predict(X_test)
acc = 100*accuracy_score(Y_test, Y_pred)
print(f"Test accuracy: {acc:.2f}%")

from sklearn.metrics import classification_report
# c-4)

dt_pred = best_dt.predict(X_test)
rf_pred = best_rf.predict(X_test)

acc_dt = 100*accuracy_score(Y_test, dt_pred)
acc_rf = 100*accuracy_score(Y_test, rf_pred)
print(f"Decision Tree Accuracy: {acc_dt:.2f}%")
print(f"Random Forest Accuracy: {acc_rf:.2f}%")
print("\nDecision Tree Classification Report:\n",
      classification_report(Y_test, dt_pred))
print("\nRandom Forest Classification Report:\n",
      classification_report(Y_test, rf_pred))

# c - 5)
# Check overfitting
models = {
    "Logistic Regression": LogisticRegression(C=0.01, penalty='l1', solver='liblinear'),
    "K-Nearest Neighbors": KNeighborsClassifier(leaf_size=30, metric='manhattan', n_neighbors=10, p=1, weights='distance'),
    "Decision Tree": DecisionTreeClassifier(criterion='gini', max_depth=3, max_features=None, min_samples_leaf=1, min_samples_split=2),
    "Random Forest": RandomForestClassifier(bootstrap=False, criterion='gini', max_depth=10, max_features='sqrt', min_samples_leaf=1, min_samples_split=20, n_estimators=100),
    "Support Vector Machine": SVC(C=0.1, gamma=1, kernel='rbf')
}

# Train each model and evaluate on training and test data
results = {}
for name, model in models.items():
    model.fit(X_train, Y_train)
    train_acc = model.score(X_train, Y_train)
    test_acc = accuracy_score(Y_test, model.predict(X_test))
    results[name] = (train_acc, test_acc)
# Display the training and testing accuracies
for name, (train_acc, test_acc) in results.items():
    print(f"{name} - Training Accuracy: {train_acc:.2f}, Test Accuracy: {test_acc:.2f}")

from sklearn.model_selection import cross_val_score
cv_results = {}
for name, model in models.items():
    cv_scores = cross_val_score(model, X_train, Y_train, cv=5)
    cv_results[name] = cv_scores
    print(f"{name} - CV Accuracy Scores: {cv_scores}")
    print(f"{name} - CV Mean Accuracy: {np.mean(cv_scores):.2f}, CV Std Deviation: {np.std(cv_scores):.2f}")

# Plotting training vs. testing accuracies
fig, ax = plt.subplots()
index = np.arange(len(results))
bar_width = 0.35

train_bars = ax.bar(index, [results[name][0]
                    for name in models.keys()], bar_width, label='Train Acc')
test_bars = ax.bar(index + bar_width, [results[name][1]
                   for name in models.keys()], bar_width, label='Test Acc')

ax.set_xlabel('Models')
ax.set_ylabel('Accuracy')
ax.set_title('Model Comparison for Overfitting')
ax.set_xticks(index + bar_width / 2)
ax.set_xticklabels(models.keys())
ax.legend()

plt.xticks(rotation=45)
plt.show()

X = patients_data.iloc[:, :-1]
Y = patients_data.iloc[:, -1]

# Split the dataset into the training set and the combined test/validation set
X_train, X_temp, Y_train, Y_temp = train_test_split(
    X, Y, test_size=0.30, random_state=42)

# Split the combined test/validation set into testing set and validation set
X_test, X_val, Y_test, Y_val = train_test_split(
    X_temp, Y_temp, test_size=0.50, random_state=42)

models = {
    "K-Nearest Neighbors": KNeighborsClassifier(leaf_size=30, metric='manhattan', n_neighbors=10, p=1, weights='distance'),
    "Decision Tree": DecisionTreeClassifier(criterion='gini', max_depth=3, max_features=None, min_samples_leaf=1, min_samples_split=2),
    "Random Forest": RandomForestClassifier(bootstrap=False, criterion='gini', max_depth=10, max_features='sqrt', min_samples_leaf=1, min_samples_split=20, n_estimators=100)
}

# Train each model and predict on validation set
results = {}
for name, model in models.items():
    model.fit(X_train, Y_train)
    Y_val_pred = model.predict(X_val)
    accuracy = accuracy_score(Y_val, Y_val_pred)
    report = classification_report(Y_val, Y_val_pred)
    conf_matrix = confusion_matrix(Y_val, Y_val_pred)
    results[name] = (accuracy, report, conf_matrix)

for name, (accuracy, report, conf_matrix) in results.items():
    print(f"{name} - Validation Accuracy: {accuracy:.2f}")
    print(f"{name} - Classification Report:\n{report}")
    print(f"{name} - Confusion Matrix:\n{conf_matrix}\n")